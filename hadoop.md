##Introduction
This set of exercises consisted of using PIG on a hadoop cluster set up using AWS EC2 resources to complete are very large scale graph analysis of parts of the billion triples dataset. We were resposible for setting up everything from scratch, thougn provided with excellent instructions. Below are the command lists used to obtain the answers to the problems. While optional, this assignmant in particular was quite interesting.

The datasets used for this analysis were portions of the [billion triples dataset](http://km.aifb.kit.edu/projects/btc-2010/). The entire dataset an RDF graph of approximately a billion triples from the Semantic Web. The RDF triples are a sets of (subject, predicate, object) [context] found from web crawling. The [context] is not actually part of the triple. A 250 kb piece of this dataset stored in S3 storage was used for testing, a 550 GB piece was used foe questions 1-3, and a 0.5 TB chunck was used for the last question.

The paragraphs below are the PIG commands used to generate the answers for each question, after connecting to the cluster, and verifying all resources were allocated and registered. [aws-cluster-info] was replaced with the appropriate cluster information.

**Question 1:** How many records are in count_by_object in the 550 GB dataset?

```sh
$ pig
fs -mkdir /user/hadoop
fs -getmerge /user/hadoop/example-results_1 example-results
scp -o "ServerAliveInterval 10" -i ~/.ssh/aws_1.pem -r hadoop[aws-cluster-info]:example-results
register s3n://uw-cse-344-oregon.aws.amazon.com/myudfs.jar
raw = LOAD 's3n://uw-cse-344-oregon.aws.amazon.com/btc-2010-chunk-000' USING TextLoader as (line:chararray);
ntriples = foreach raw generate FLATTEN(myudfs.RDFSplit3(line)) as (subject:chararray,predicate:chararray,object:chararray);
objects = group ntriples by (object) PARALLEL 50;
count_by_object = foreach objects generate flatten($0), COUNT($1) as count PARALLEL 50;
count_by_object_ordered = order count_by_object by (count)  PARALLEL 50;
store count_by_object_ordered into '/user/hadoop/example-results_1' using PigStorage();
```

**Question 2:** Generate a "histogram" of the counts of subjects. The "x-axis" is the counts of subjects, and the "y-axis" should be the the total number of subjects associate with each count. Thus each x-y pair is y subjects had x tuples associated with that subject. The result to be tured in is the total number of points generated in this manner.

```sh
$ pig
fs -mkdir /user/hadoop
fs -getmerge /user/hadoop/problem2 problem2
scp -o "ServerAliveInterval 10" -i ~/.ssh/aws_1.pem -r hadoop[aws-cluster-info]:problem2
register s3n://uw-cse-344-oregon.aws.amazon.com/myudfs.jar
raw = LOAD 's3n://uw-cse-344-oregon.aws.amazon.com/btc-2010-chunk-000' USING TextLoader as (line:chararray);
ntriples = foreach raw generate FLATTEN(myudfs.RDFSplit3(line)) as (subject:chararray,predicate:chararray,object:chararray);
subjects = group ntriples by (subject) PARALLEL 50;
count_by_subject = foreach subjects generate flatten($0), COUNT($1) as count1 PARALLEL 50;
counts = group count_by_subject by (count1) PARALLEL 50;
final_count = foreach counts generate flatten($0), COUNT($1) as next_count PARALLEL 50;
x = group final_count all;
total_num = foreach x generate COUNT(final_count);
store total_num into '/user/hadoop/problem2b' using PigStorage();
```

**Question 3:** Find all chains of length 2 in the subgraph consisting of triples whose subject matches rdfabout.com. Return the number of records generated by finding all sextuples (subject, predicate, object, subject2, predicate2, object2) where object=object2. This is effectvely a join operation.

```sh
$ pig
fs -mkdir /user/hadoop
fs -getmerge /user/hadoop/problem3 problem3
scp -o "ServerAliveInterval 10" -i ~/.ssh/aws_1.pem -r hadoop[aws-cluster-info]:problem3
register s3n://uw-cse-344-oregon.aws.amazon.com/myudfs.jar
raw = LOAD 's3n://uw-cse-344-oregon.aws.amazon.com/btc-2010-chunk-000' USING TextLoader as (line:chararray);
ntriples = foreach raw generate FLATTEN(myudfs.RDFSplit3(line)) as (subject:chararray,predicate:chararray,object:chararray);
filtered = FILTER ntriples BY (subject matches '.*rdfabout\\.com.*');
filtered2 = foreach filtered generate subject as subject2, predicate as predicate2, object as object2;
joined = join filtered by object, filtered2 by subject2;
distinct_values = distinct joined;
x2 = group distinct_values all;
total_num2 = foreach x2 generate COUNT(distinct_values);
store total_num2 into '/user/hadoop/problem3b' using PigStorage();
```

**Question 4:** The same as question 2, but on the 0.5 TB dataset.
